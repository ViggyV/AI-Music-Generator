{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN 3.0\n",
    "Through my research, I was able to decided to use a GAN to create music. I would first convert .wav files to pngs and use that throughout my GAN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The import statement above is used for some prints and special uses from tensorflow beta 2.0. Below of the required import statements to run and use the GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import glob#finds pathnames\n",
    "import imageio#handles image input and output\n",
    "import matplotlib.pyplot as plt#data visulization\n",
    "import numpy as np#obivous array and math uses\n",
    "import os#helps uses files across different os\n",
    "import PIL#pictures\n",
    "import numpy\n",
    "import math\n",
    "from PIL import Image\n",
    "from scipy.io.wavfile import read, write\n",
    "from tensorflow.keras import layers#layers in the neural nets\n",
    "import time#to figure out timings\n",
    "from tqdm import tqdm\n",
    "\n",
    "from IPython import display#will show the resulting converted pngs\n",
    "BATCH_SIZE = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputting .wav files and Converting them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out\n",
      "vs1-2fug.wav\n",
      "work\n",
      "after png\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-46317ff57bf3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# split all 16-bit integers in WAV file to 2x 8 bit integers;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# 1 16 bit int per pixel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0md_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>>\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;36m0xff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;36m0xff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0md_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>>\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;36m0xff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;36m0xff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mpng_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "os.chdir(\"/Users/viggy/Documents/GitHub/AI-Music-Generator/ViolinMIDI/bwv1001\")\n",
    "# read the WAV data\n",
    "wavs_in = []\n",
    "png_data = []\n",
    "song=0\n",
    "print(\"out\")\n",
    "\n",
    "for filename in glob.glob(\"*.wav\"):\n",
    "    print(filename)\n",
    "    print(\"work\")\n",
    "    wavs_in.append(read(filename))\n",
    "    wav_data = list(wavs_in[song][1])\n",
    "    #print(list(wavs_in[song]))\n",
    "    png_data = []\n",
    "    print(\"after png\")\n",
    "    for d in wav_data:\n",
    "        # split all 16-bit integers in WAV file to 2x 8 bit integers;\n",
    "        # 1 16 bit int per pixel\n",
    "        d_1 = (((d[0] >> 8) & 0xff), d[0] & 0xff, 0)\n",
    "        d_2 = (((d[1] >> 8) & 0xff), d[1] & 0xff, 0)\n",
    "        png_data.append(d_1)\n",
    "        png_data.append(d_2)\n",
    "    print(\"append\")\n",
    "    # ending indicator pixel uses the green channel\n",
    "    # (green channel set to 0 for all data pixels)\n",
    "    png_data += [(255, 255, 255)]\n",
    "    # find a roughly square size of output image\n",
    "    n = len(png_data)\n",
    "    x = math.floor(math.sqrt(n))\n",
    "    y = x + math.ceil((n - x ** 2) / x)\n",
    "\n",
    "    # output the PNG image\n",
    "    img = PIL.Image.new('RGB', (int(x), int(y)), color = 'white')\n",
    "    img.putdata(png_data)\n",
    "    img.save(filename.replace(\".wav\",\".png\"))\n",
    "    song+=1\n",
    "print(\"fin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(img):\n",
    "    print(\"start cropping\")\n",
    "    images=[]\n",
    "    print(img)\n",
    "    #image_obj = Image.open(image_path)\n",
    "    #%pylab inline\n",
    "    #import matplotlib.pyplot as plt\n",
    "    #import matplotlib.image as mpimg\n",
    "    #img=mpimg.imread('your_image.png')\n",
    "    #imgplot = plt.imshow(img)\n",
    "    #plt.show()\n",
    "    current_x=0\n",
    "    current_y=0\n",
    "    next_x=28\n",
    "    next_y=28\n",
    "    coords=[current_x,current_y,next_x,next_y]\n",
    "    count=30\n",
    "    i=0\n",
    "    pbar = tqdm(total=5)\n",
    "    while i<=30 and next_y < image.shape[1]:\n",
    "        while next_x < image.shape[0]:\n",
    "            print(hawk)\n",
    "            cropped_image = image\n",
    "            #cropped_image.save(saved_location)\n",
    "            images.append(cropped_image)\n",
    "            coords=[current_x+28,current_y,next_x+28,next_y]\n",
    "            i+=1\n",
    "        current_x,current_y,next_x,next_y=0\n",
    "        coords=[current_x,current_y,next_x+28,next_y+28]\n",
    "        pbar.update(1)\n",
    "    \n",
    "    pbar.close()\n",
    "    cropped_image.show()\n",
    "    print(\"Done Cropping\")\n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_model():\n",
    "\n",
    "    model=tf.keras.Sequential()\n",
    "    model.add(layers.Dense(7*7*256,use_bias=True,input_shape=(100,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ReLU())\n",
    "\n",
    "    model.add(layers.Reshape((7,7,256)))\n",
    "    assert model.output_shape==(None,7,7,256)\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(128,(5,5),strides=(1,1),padding='same',use_bias=True))\n",
    "    assert model.output_shape==(None,7,7,128)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ReLU())\n",
    "    \n",
    "    \n",
    "    model.add(layers.Conv2DTranspose(64,(5,5),strides=(2,2),padding='same',use_bias=True))\n",
    "    assert model.output_shape==(None,14,14,64)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ReLU())\n",
    "    \n",
    "    model.add(layers.Conv2DTranspose(1,(5,5),strides=(2,2),padding='same',use_bias=True,activation='sigmoid'))\n",
    "    assert model.output_shape==(None,28,28,1)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0xbda605b00>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "generator=generator_model()\n",
    "\n",
    "noise=tf.random.normal([1, 100])\n",
    "generated_image=generator(noise,training=False)\n",
    "\n",
    "plt.imshow(generated_image[0, :, :, 0],cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_model():\n",
    "    model=tf.keras.Sequential()\n",
    "    model.add(layers.Conv2D(64,(5,5),strides=(2,2),padding=\"same\",input_shape=[28,28,1]))\n",
    "    model.add(layers.ReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(layers.Conv2D(128,(5,5),strides=(2,2),padding='same'))\n",
    "    model.add(layers.ReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[-0.05274365]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "discriminator=discriminator_model()\n",
    "decision=discriminator(generated_image)\n",
    "print(decision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Losses and Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keras helper function to compute cross entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,discriminator_optimizer=discriminator_optimizer,generator=generator,discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "noise_dim = 100\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "# We will reuse this seed overtime (so it's easier)\n",
    "# to visualize progress in the animated GIF)\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "      generated_images = generator(noise, training=True)\n",
    "\n",
    "      real_output = discriminator(images, training=True)\n",
    "      fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "      gen_loss = generator_loss(fake_output)\n",
    "      disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "  for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    for image_batch in dataset:\n",
    "      train_step(image_batch)\n",
    "\n",
    "    # Produce images for the GIF as we go\n",
    "    display.clear_output(wait=True)\n",
    "    generate_and_save_images(generator,\n",
    "                             epoch + 1,\n",
    "                             seed)\n",
    "\n",
    "    # Save the model every 15 epochs\n",
    "    if (epoch + 1) % 15 == 0:\n",
    "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "\n",
    "  # Generate after the final epoch\n",
    "  display.clear_output(wait=True)\n",
    "  generate_and_save_images(generator,epochs,seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input):\n",
    "  # Notice `training` is set to False.\n",
    "  # This is so all layers run in inference mode (batchnorm).\n",
    "  predictions = model(test_input, training=False)\n",
    "\n",
    "  fig = plt.figure(figsize=(4,4))\n",
    "\n",
    "  for i in range(predictions.shape[0]):\n",
    "      plt.subplot(4, 4, i+1)\n",
    "      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "      plt.axis('off')\n",
    "\n",
    "  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reshaped\n",
      "(4246, 4245, 3)\n",
      "[array([[0.        , 0.        , 0.        , ..., 0.99607843, 0.99215686,\n",
      "        0.99607843],\n",
      "       [0.99215686, 0.99607843, 0.99215686, ..., 0.02352941, 0.01960784,\n",
      "        0.02352941],\n",
      "       [0.01960784, 0.02352941, 0.02352941, ..., 0.01176471, 0.00784314,\n",
      "        0.00784314],\n",
      "       ...,\n",
      "       [0.02352941, 0.00784314, 0.02745098, ..., 0.01568628, 0.99607843,\n",
      "        0.01568628],\n",
      "       [0.99607843, 0.01568628, 0.99607843, ..., 0.00784314, 0.01176471,\n",
      "        0.00784314],\n",
      "       [0.00784314, 0.00392157, 0.        , ..., 1.        , 1.        ,\n",
      "        1.        ]], dtype=float32), array([[0.        , 0.        , 0.        , ..., 0.49019608, 0.3529412 ,\n",
      "        0.47843137],\n",
      "       [0.6156863 , 0.6117647 , 0.9529412 , ..., 0.30980393, 0.6       ,\n",
      "        0.64705884],\n",
      "       [0.9647059 , 0.81960785, 0.14509805, ..., 0.5647059 , 0.30980393,\n",
      "        0.96862745],\n",
      "       ...,\n",
      "       [0.04705882, 0.09411765, 0.37254903, ..., 0.69803923, 0.00784314,\n",
      "        0.34509805],\n",
      "       [0.38431373, 0.05882353, 0.45882353, ..., 0.65882355, 0.78431374,\n",
      "        0.30980393],\n",
      "       [0.1882353 , 0.84313726, 0.7529412 , ..., 1.        , 1.        ,\n",
      "        1.        ]], dtype=float32), array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 1., 1., 1.]], dtype=float32)]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-32bebf259cad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m#uploaded = im.reshape(im.shape[0], 28, 28, 3).astype('float32')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m#uploaded = (uploaded - 127.5) / 127.5 # Normalize the images to [-1, 1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "from scipy import ndimage, misc\n",
    "filepath=\"/Users/viggy/Documents/GitHub/AI-Music-Generator/ViolinMIDI/bwv1001/vs1-1ada.png\"\n",
    "#data=plt.imread(filepath)\n",
    "#data.shape\n",
    "#mage_resized = misc.imresize(image, (64, 64))\n",
    "#data = data.reshape(28, 28, 1).astype('float32')\n",
    "#data = (data - 127.5) / 127.5 # Normalize the images to [-1, 1]\n",
    "images = []\n",
    "#print(\"going to decode\")\n",
    "#image=tf.image.decode_png(Image.open(filepath))\n",
    "#print(\"decode\")\n",
    "#images=image.reshape(uploaded.shape[0],28,28,1).astype('float32')\n",
    "#uploaded=(images-127.5)/127.5\n",
    "#images=crop(\"/Users/viggy/Documents/GitHub/AI-Music-Generator/ViolinMIDI/bwv1001/vs1-1ada.png\",\n",
    "       #    \"/Users/viggy/Documents/GitHub/AI-Music-Generator/ViolinMIDI/bwv1001/vs1-1ada\")\n",
    "print(\"reshaped\")\n",
    "#images.append(image_resized)\n",
    "#%%time\n",
    "im = plt.imread(filepath)\n",
    "print(im.shape)\n",
    "#uploaded=tf.image.decode_png(Image.open(filepath))\n",
    "#crop(im)\n",
    "import cv2\n",
    "colors=[]\n",
    "colors=cv2.split(im)\n",
    "#uploaded = im.reshape(im.shape[0], 28, 28, 3).astype('float32')\n",
    "#uploaded = (uploaded - 127.5) / 127.5 # Normalize the images to [-1, 1]\n",
    "train(colors, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a single image using the epoch number\n",
    "def display_image(epoch_no):\n",
    "  return PIL.Image.open('image_at_epoch_{:04d}.png'.format(epoch_no))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'display_image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-7d0477526f8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdisplay_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'display_image' is not defined"
     ]
    }
   ],
   "source": [
    "display_image(EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim_file = 'dcgan.gif'\n",
    "\n",
    "with imageio.get_writer(anim_file, mode='I') as writer:\n",
    "  filenames = glob.glob('image*.png')\n",
    "  filenames = sorted(filenames)\n",
    "  last = -1\n",
    "  for i,filename in enumerate(filenames):\n",
    "    frame = 2*(i**0.5)\n",
    "    if round(frame) > round(last):\n",
    "      last = frame\n",
    "    else:\n",
    "      continue\n",
    "    image = imageio.imread(filename)\n",
    "    writer.append_data(image)\n",
    "  image = imageio.imread(filename)\n",
    "  writer.append_data(image)\n",
    "\n",
    "import IPython\n",
    "if IPython.version_info > (6,2,0,''):\n",
    "  display.Image(filename=anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import math\n",
    "from PIL import Image\n",
    "from scipy.io.wavfile import read, write\n",
    "\n",
    "# read in the PNG file\n",
    "img = Image.open(\"/Users/viggy/Documents/GitHub/AI-Music-Generator/ViolinMIDI/bwv1001/vs1-1ada.png\")\n",
    "\n",
    "# look for the end indicator pixel and trim accordingly\n",
    "png_data = list(img.getdata())\n",
    "for i in range(len(png_data)):\n",
    "    if png_data[i] == [255, 255, 255]: # look for end pixel\n",
    "        png_data = png_data[:i]\n",
    "        break\n",
    "\n",
    "# convert data into WAV format\n",
    "wav_data = []\n",
    "for i in range(0, len(png_data), 2):\n",
    "    # read the first integer from 2x 8 bit integers in the first pixel,\n",
    "    # and the second from 2x 8 bit integers in the second pixel\n",
    "    wav_data.append([(png_data[i][0] << 8) + png_data[i][1], \n",
    "                     (png_data[i+1][0] << 8) + png_data[i+1][1]]) \n",
    "\n",
    "# output the WAV audio\n",
    "write('output.wav', 44100, numpy.asarray(wav_data, dtype=numpy.int16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
