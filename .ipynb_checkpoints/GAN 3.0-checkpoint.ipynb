{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN 3.0\n",
    "Through my research, I was able to decided to use a GAN to create music. I would first convert .wav files to pngs and use that throughout my GAN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The import statement above is used for some prints and special uses from tensorflow beta 2.0. Below of the required import statements to run and use the GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import glob#finds pathnames\n",
    "import imageio#handles image input and output\n",
    "import matplotlib.pyplot as plt#data visulization\n",
    "import numpy as np#obivous array and math uses\n",
    "import os#helps uses files across different os\n",
    "import PIL#pictures\n",
    "import numpy\n",
    "import math\n",
    "from PIL import Image\n",
    "from scipy.io.wavfile import read, write\n",
    "from tensorflow.keras import layers#layers in the neural nets\n",
    "import time#to figure out timings\n",
    "from tqdm import tqdm\n",
    "import image_slicer\n",
    "from pathlib import Path\n",
    "from IPython import display#will show the resulting converted pngs\n",
    "BATCH_SIZE = 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputting .wav files and Converting them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out\n",
      "out.wav\n",
      "work\n"
     ]
    }
   ],
   "source": [
    "\n",
    "os.chdir(\"/Users/viggy/Documents/GitHub/AI-Music-Generator/ViolinMIDI/bwv1001\")\n",
    "# read the WAV data\n",
    "wavs_in = []\n",
    "png_data = []\n",
    "song=0\n",
    "print(\"out\")\n",
    "\n",
    "for filename in glob.glob(\"*.wav\"):\n",
    "    print(filename)\n",
    "    print(\"work\")\n",
    "    wavs_in.append(read(filename))\n",
    "    wav_data = list(wavs_in[song][1])\n",
    "    #print(list(wavs_in[song]))\n",
    "    png_data = []\n",
    "    print(\"after png\")\n",
    "    for d in wav_data:\n",
    "        # split all 16-bit integers in WAV file to 2x 8 bit integers;\n",
    "        # 1 16 bit int per pixel\n",
    "        d_1 = (((d[0] >> 8) & 0xff), d[0] & 0xff, 0)\n",
    "        d_2 = (((d[1] >> 8) & 0xff), d[1] & 0xff, 0)\n",
    "        png_data.append(d_1)\n",
    "        png_data.append(d_2)\n",
    "    print(\"append\")\n",
    "    # ending indicator pixel uses the green channel\n",
    "    # (green channel set to 0 for all data pixels)\n",
    "    png_data += [(255, 255, 255)]\n",
    "    # find a roughly square size of output image\n",
    "    n = len(png_data)\n",
    "    x = math.floor(math.sqrt(n))\n",
    "    y = x + math.ceil((n - x ** 2) / x)\n",
    "\n",
    "    # output the PNG image\n",
    "    img = PIL.Image.new('RGB', (int(x), int(y)), color = 'white')\n",
    "    img.putdata(png_data)\n",
    "    img.save(filename.replace(\".wav\",\".png\"))\n",
    "    song+=1\n",
    "print(\"fin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(img):\n",
    "    print(\"start cropping\")\n",
    "    images=[]\n",
    "    print(img)\n",
    "    #image_obj = Image.open(image_path)\n",
    "    #%pylab inline\n",
    "    #import matplotlib.pyplot as plt\n",
    "    #import matplotlib.image as mpimg\n",
    "    #img=mpimg.imread('your_image.png')\n",
    "    #imgplot = plt.imshow(img)\n",
    "    #plt.show()\n",
    "    current_x=0\n",
    "    current_y=0\n",
    "    next_x=28\n",
    "    next_y=28\n",
    "    coords=[current_x,current_y,next_x,next_y]\n",
    "    count=30\n",
    "    i=0\n",
    "    pbar = tqdm(total=5)\n",
    "    while i<=30 and next_y < image.shape[1]:\n",
    "        while next_x < image.shape[0]:\n",
    "            print(hawk)\n",
    "            cropped_image = image\n",
    "            #cropped_image.save(saved_location)\n",
    "            images.append(cropped_image)\n",
    "            coords=[current_x+28,current_y,next_x+28,next_y]\n",
    "            i+=1\n",
    "        current_x,current_y,next_x,next_y=0\n",
    "        coords=[current_x,current_y,next_x+28,next_y+28]\n",
    "        pbar.update(1)\n",
    "    \n",
    "    pbar.close()\n",
    "    cropped_image.show()\n",
    "    print(\"Done Cropping\")\n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_model():\n",
    "\n",
    "    model=tf.keras.Sequential()\n",
    "    model.add(layers.Dense(7*7*256,use_bias=True,input_shape=(100,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ReLU())\n",
    "\n",
    "    model.add(layers.Reshape((7,7,256)))\n",
    "    assert model.output_shape==(None,7,7,256)\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(128,(5,5),strides=(1,1),padding='same',use_bias=True))\n",
    "    assert model.output_shape==(None,7,7,128)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ReLU())\n",
    "    \n",
    "    \n",
    "    model.add(layers.Conv2DTranspose(64,(5,5),strides=(2,2),padding='same',use_bias=True))\n",
    "    assert model.output_shape==(None,14,14,64)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ReLU())\n",
    "    \n",
    "    model.add(layers.Conv2DTranspose(1,(5,5),strides=(2,2),padding='same',use_bias=True,activation='sigmoid'))\n",
    "    assert model.output_shape==(None,28,28,1)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x10cc86908>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "generator=generator_model()\n",
    "\n",
    "noise=tf.random.normal([1, 100])\n",
    "generated_image=generator(noise,training=False)\n",
    "\n",
    "plt.imshow(generated_image[0, :, :, 0],cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_model():\n",
    "    model=tf.keras.Sequential()\n",
    "    model.add(layers.Conv2D(64,(5,5),strides=(2,2),padding=\"same\",input_shape=[28,28,1]))\n",
    "    model.add(layers.ReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(layers.Conv2D(128,(5,5),strides=(2,2),padding='same'))\n",
    "    model.add(layers.ReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.05481029]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "discriminator=discriminator_model()\n",
    "decision=discriminator(generated_image)\n",
    "print(decision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Losses and Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keras helper function to compute cross entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,discriminator_optimizer=discriminator_optimizer,generator=generator,discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "noise_dim = 100\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "# We will reuse this seed overtime (so it's easier)\n",
    "# to visualize progress in the animated GIF)\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "    print(\"in train_step\")\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "      generated_images = generator(noise, training=True)\n",
    "\n",
    "      real_output = discriminator(images, training=True)\n",
    "      fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "      gen_loss = generator_loss(fake_output)\n",
    "      disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "  for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    print(\"Epoch\" )\n",
    "    print(epochs)\n",
    "    for image_batch in dataset:\n",
    "      train_step(image_batch)\n",
    "\n",
    "    # Produce images for the GIF as we go\n",
    "    display.clear_output(wait=True)\n",
    "    generate_and_save_images(generator,\n",
    "                             epoch + 1,\n",
    "                             seed)\n",
    "\n",
    "    # Save the model every 15 epochs\n",
    "    if (epoch + 1) % 15 == 0:\n",
    "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "\n",
    "  # Generate after the final epoch\n",
    "  display.clear_output(wait=True)\n",
    "  generate_and_save_images(generator,epochs,seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input):\n",
    "  # Notice `training` is set to False.\n",
    "  # This is so all layers run in inference mode (batchnorm).\n",
    "  predictions = model(test_input, training=False)\n",
    "\n",
    "  fig = plt.figure(figsize=(4,4))\n",
    "\n",
    "  for i in range(predictions.shape[0]):\n",
    "      plt.subplot(4, 4, i+1)\n",
    "      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "      plt.axis('off')\n",
    "\n",
    "  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(img,time):\n",
    "    os.chdir(\"/Users/viggy/Documents/GitHub/AI-Music-Generator/ViolinMIDI/bwv1001/vs1-ada\")\n",
    "    every_second=image_slicer.slice(img, time)\n",
    "    return every_second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_1():\n",
    "    ala_path=\"/Users/viggy/Documents/GitHub/AI-Music-Generator/ViolinMIDI/bwv1001/vs1-ada\"\n",
    "    os.chdir(\"/Users/viggy/Documents/GitHub/AI-Music-Generator/ViolinMIDI/bwv1001/vs1-ada\")\n",
    "    png_data = []\n",
    "    for filename in glob.glob(\"*.png\"):\n",
    "        png_data.append(np.array(Image.open(ala_path+\"/\"+filename)))\n",
    "    dataset1=np.array(png_data)\n",
    "    return dataset1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reshaped\n",
      "(4246, 4245, 3)\n",
      "(283, 283, 3)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 54060075 into shape (225,28,28,3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-cc771046afcf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m#print(dataset1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;31m#uploaded = (uploaded - 127.5) / 127.5 # Normalize the images to [-1, 1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muploaded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 54060075 into shape (225,28,28,3)"
     ]
    }
   ],
   "source": [
    "\n",
    "from scipy import ndimage, misc\n",
    "filepath=\"/Users/viggy/Documents/GitHub/AI-Music-Generator/ViolinMIDI/bwv1001/vs1-1ada.png\"\n",
    "#data=plt.imread(filepath)\n",
    "#data.shape\n",
    "#mage_resized = misc.imresize(image, (64, 64))\n",
    "#data = data.reshape(28, 28, 1).astype('float32')\n",
    "#data = (data - 127.5) / 127.5 # Normalize the images to [-1, 1]\n",
    "images = []\n",
    "\n",
    "ala_path=\"/Users/viggy/Documents/GitHub/AI-Music-Generator/ViolinMIDI/bwv1001/vs1-ada\"\n",
    "print(\"reshaped\")\n",
    "#images.append(image_resized)\n",
    "#%%time\n",
    "im = plt.imread(filepath)\n",
    "print(im.shape)\n",
    "#uploaded=tf.image.decode_png(Image.open(filepath))\n",
    "#crop(im)\n",
    "#import cv2\n",
    "#colors=[]\n",
    "#colors=cv2.split(im)\n",
    "#split((filepath),224)\n",
    "#print(every_second)\n",
    "#uploaded = every_second.resize(every_second.shape[0], 28, 28, 3).astype('float32')\n",
    "\n",
    "dataset1=dataset_1()\n",
    "dataset1=np.array(dataset1)\n",
    "print(dataset1[0].shape)\n",
    "\n",
    "#print(dataset1)\n",
    "uploaded = dataset1.reshape(dataset1.shape[0], 283, 283, 3).astype('float32')\n",
    "#uploaded = (uploaded - 127.5) / 127.5 # Normalize the images to [-1, 1]\n",
    "train(uploaded, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a single image using the epoch number\n",
    "def display_image(epoch_no):\n",
    "  return PIL.Image.open('image_at_epoch_{:04d}.png'.format(epoch_no))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'image_at_epoch_0050.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-151-7d0477526f8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdisplay_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-150-2f3d8de0ec11>\u001b[0m in \u001b[0;36mdisplay_image\u001b[0;34m(epoch_no)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Display a single image using the epoch number\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdisplay_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'image_at_epoch_{:04d}.png'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2634\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2635\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'image_at_epoch_0050.png'"
     ]
    }
   ],
   "source": [
    "display_image(EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim_file = 'dcgan.gif'\n",
    "\n",
    "with imageio.get_writer(anim_file, mode='I') as writer:\n",
    "  filenames = glob.glob('image*.png')\n",
    "  filenames = sorted(filenames)\n",
    "  last = -1\n",
    "  for i,filename in enumerate(filenames):\n",
    "    frame = 2*(i**0.5)\n",
    "    if round(frame) > round(last):\n",
    "      last = frame\n",
    "    else:\n",
    "      continue\n",
    "    image = imageio.imread(filename)\n",
    "    writer.append_data(image)\n",
    "  image = imageio.imread(filename)\n",
    "  writer.append_data(image)\n",
    "\n",
    "import IPython\n",
    "if IPython.version_info > (6,2,0,''):\n",
    "  display.Image(filename=anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import math\n",
    "from PIL import Image\n",
    "from scipy.io.wavfile import read, write\n",
    "\n",
    "# read in the PNG file\n",
    "img = Image.open(\"/Users/viggy/Documents/GitHub/AI-Music-Generator/ViolinMIDI/bwv1001/vs1-1ada.png\")\n",
    "\n",
    "# look for the end indicator pixel and trim accordingly\n",
    "png_data = list(img.getdata())\n",
    "for i in range(len(png_data)):\n",
    "    if png_data[i] == [255, 255, 255]: # look for end pixel\n",
    "        png_data = png_data[:i]\n",
    "        break\n",
    "\n",
    "# convert data into WAV format\n",
    "wav_data = []\n",
    "for i in range(0, len(png_data), 2):\n",
    "    # read the first integer from 2x 8 bit integers in the first pixel,\n",
    "    # and the second from 2x 8 bit integers in the second pixel\n",
    "    wav_data.append([(png_data[i][0] << 8) + png_data[i][1], \n",
    "                     (png_data[i+1][0] << 8) + png_data[i+1][1]]) \n",
    "\n",
    "# output the WAV audio\n",
    "write('output.wav', 44100, numpy.asarray(wav_data, dtype=numpy.int16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
